{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOA/8AkDy/AO2vfPKe1Mgqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import KFold\n","import numpy as np\n","import time\n","\n","transform_mnist = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","transform_cifar = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n","mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n","\n","cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n","cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, layer_sizes, output_size, dropout):\n","        super().__init__()\n","        layers = []\n","        prev = input_size\n","        for size in layer_sizes:\n","            layers.append(nn.Linear(prev, size))\n","            layers.append(nn.ReLU())\n","            if dropout > 0:\n","                layers.append(nn.Dropout(dropout))\n","            prev = size\n","        layers.append(nn.Linear(prev, output_size))\n","        self.net = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        return self.net(x)\n","\n","def train_model(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        loss = criterion(model(x), y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def evaluate_model(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            preds = model(x).argmax(dim=1)\n","            correct += (preds == y).sum().item()\n","            total += y.size(0)\n","    return correct / total\n","\n","def run_cv(dataset, input_size, layer_sizes, output_size, search_space, folds=3, device='cpu'):\n","    kf = KFold(n_splits=folds, shuffle=True, random_state=0)\n","    indices = list(range(len(dataset)))\n","    results = []\n","\n","    for config in search_space:\n","        acc_scores = []\n","        start_time = time.time()\n","\n","        print(f\"\\n=== Config: {config}, Architecture: {layer_sizes} ===\")\n","\n","        for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","            print(f\"\\n--- Fold {fold + 1}/{folds} ---\")\n","\n","            train_data = Subset(dataset, train_idx)\n","            val_data = Subset(dataset, val_idx)\n","\n","            train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n","            val_loader = DataLoader(val_data, batch_size=config['batch_size'], shuffle=False)\n","\n","            model = MLP(input_size, layer_sizes, output_size, config['dropout']).to(device)\n","            optimizer = optim.SGD(model.parameters(), lr=config['lr']) if config['optimizer'] == 'SGD' \\\n","                        else optim.Adam(model.parameters(), lr=config['lr'])\n","            criterion = nn.CrossEntropyLoss()\n","\n","            for epoch in range(15):\n","                train_loss = train_model(model, train_loader, optimizer, criterion, device)\n","                val_acc = evaluate_model(model, val_loader, device)\n","                print(f\"Epoch {epoch+1:02d}/15 - Train Loss: {train_loss:.4f} - Val Acc: {val_acc:.4f}\")\n","\n","            final_acc = evaluate_model(model, val_loader, device)\n","            acc_scores.append(final_acc)\n","\n","        mean_acc = np.mean(acc_scores)\n","        std_acc = np.std(acc_scores)\n","        runtime = time.time() - start_time\n","\n","        results.append({\n","            'architecture': layer_sizes,\n","            'config': config,\n","            'mean_acc': mean_acc,\n","            'std_acc': std_acc,\n","            'runtime_sec': runtime\n","        })\n","\n","    return results\n","\n","shallow = [128]\n","medium = [512, 256, 128]\n","deep = [4096, 2048, 1024, 512, 256, 128, 64]\n","\n","search_space = [\n","    {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0},\n","    {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2},\n","    {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}\n","]\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#MNIST\n","input_mnist = 28 * 28\n","output_mnist = 10\n","mnist_results = []\n","for arch in [shallow, medium, deep]:\n","    mnist_results.extend(run_cv(mnist_train, input_mnist, arch, output_mnist, search_space, device=device))\n","\n","#CIFAR-10\n","input_cifar = 3 * 32 * 32\n","output_cifar = 10\n","cifar_results = []\n","for arch in [shallow, medium, deep]:\n","    cifar_results.extend(run_cv(cifar_train, input_cifar, arch, output_cifar, search_space, device=device))\n","\n","import pandas as pd\n","\n","print(\"\\nMNIST Results:\")\n","mnist_df = pd.DataFrame(mnist_results)\n","print(mnist_df[['architecture', 'config', 'mean_acc', 'std_acc', 'runtime_sec']])\n","\n","print(\"\\nCIFAR-10 Results:\")\n","cifar_df = pd.DataFrame(cifar_results)\n","print(cifar_df[['architecture', 'config', 'mean_acc', 'std_acc', 'runtime_sec']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3lTCO5Jz0ly","executionInfo":{"status":"ok","timestamp":1745109259935,"user_tz":300,"elapsed":8184979,"user":{"displayName":"jerin","userId":"05872963528617007031"}},"outputId":"8832b3d7-c22a-4da5-c4bb-c8cabc75210c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.5277 - Val Acc: 0.8316\n","Epoch 02/15 - Train Loss: 0.6395 - Val Acc: 0.8692\n","Epoch 03/15 - Train Loss: 0.4670 - Val Acc: 0.8857\n","Epoch 04/15 - Train Loss: 0.4038 - Val Acc: 0.8932\n","Epoch 05/15 - Train Loss: 0.3699 - Val Acc: 0.9002\n","Epoch 06/15 - Train Loss: 0.3480 - Val Acc: 0.9041\n","Epoch 07/15 - Train Loss: 0.3318 - Val Acc: 0.9080\n","Epoch 08/15 - Train Loss: 0.3187 - Val Acc: 0.9090\n","Epoch 09/15 - Train Loss: 0.3076 - Val Acc: 0.9130\n","Epoch 10/15 - Train Loss: 0.2979 - Val Acc: 0.9149\n","Epoch 11/15 - Train Loss: 0.2892 - Val Acc: 0.9179\n","Epoch 12/15 - Train Loss: 0.2810 - Val Acc: 0.9196\n","Epoch 13/15 - Train Loss: 0.2733 - Val Acc: 0.9221\n","Epoch 14/15 - Train Loss: 0.2662 - Val Acc: 0.9238\n","Epoch 15/15 - Train Loss: 0.2592 - Val Acc: 0.9250\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.5058 - Val Acc: 0.8340\n","Epoch 02/15 - Train Loss: 0.6361 - Val Acc: 0.8740\n","Epoch 03/15 - Train Loss: 0.4682 - Val Acc: 0.8872\n","Epoch 04/15 - Train Loss: 0.4047 - Val Acc: 0.8953\n","Epoch 05/15 - Train Loss: 0.3703 - Val Acc: 0.9013\n","Epoch 06/15 - Train Loss: 0.3474 - Val Acc: 0.9040\n","Epoch 07/15 - Train Loss: 0.3309 - Val Acc: 0.9078\n","Epoch 08/15 - Train Loss: 0.3173 - Val Acc: 0.9115\n","Epoch 09/15 - Train Loss: 0.3058 - Val Acc: 0.9136\n","Epoch 10/15 - Train Loss: 0.2957 - Val Acc: 0.9161\n","Epoch 11/15 - Train Loss: 0.2865 - Val Acc: 0.9157\n","Epoch 12/15 - Train Loss: 0.2783 - Val Acc: 0.9202\n","Epoch 13/15 - Train Loss: 0.2705 - Val Acc: 0.9227\n","Epoch 14/15 - Train Loss: 0.2631 - Val Acc: 0.9242\n","Epoch 15/15 - Train Loss: 0.2561 - Val Acc: 0.9265\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.5100 - Val Acc: 0.8275\n","Epoch 02/15 - Train Loss: 0.6202 - Val Acc: 0.8710\n","Epoch 03/15 - Train Loss: 0.4550 - Val Acc: 0.8844\n","Epoch 04/15 - Train Loss: 0.3955 - Val Acc: 0.8912\n","Epoch 05/15 - Train Loss: 0.3639 - Val Acc: 0.8959\n","Epoch 06/15 - Train Loss: 0.3429 - Val Acc: 0.9004\n","Epoch 07/15 - Train Loss: 0.3273 - Val Acc: 0.9050\n","Epoch 08/15 - Train Loss: 0.3147 - Val Acc: 0.9073\n","Epoch 09/15 - Train Loss: 0.3037 - Val Acc: 0.9105\n","Epoch 10/15 - Train Loss: 0.2940 - Val Acc: 0.9128\n","Epoch 11/15 - Train Loss: 0.2855 - Val Acc: 0.9146\n","Epoch 12/15 - Train Loss: 0.2776 - Val Acc: 0.9171\n","Epoch 13/15 - Train Loss: 0.2699 - Val Acc: 0.9194\n","Epoch 14/15 - Train Loss: 0.2630 - Val Acc: 0.9225\n","Epoch 15/15 - Train Loss: 0.2559 - Val Acc: 0.9229\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 0.4369 - Val Acc: 0.9346\n","Epoch 02/15 - Train Loss: 0.2129 - Val Acc: 0.9520\n","Epoch 03/15 - Train Loss: 0.1577 - Val Acc: 0.9580\n","Epoch 04/15 - Train Loss: 0.1270 - Val Acc: 0.9647\n","Epoch 05/15 - Train Loss: 0.1051 - Val Acc: 0.9681\n","Epoch 06/15 - Train Loss: 0.0914 - Val Acc: 0.9710\n","Epoch 07/15 - Train Loss: 0.0794 - Val Acc: 0.9716\n","Epoch 08/15 - Train Loss: 0.0690 - Val Acc: 0.9716\n","Epoch 09/15 - Train Loss: 0.0612 - Val Acc: 0.9728\n","Epoch 10/15 - Train Loss: 0.0551 - Val Acc: 0.9736\n","Epoch 11/15 - Train Loss: 0.0500 - Val Acc: 0.9737\n","Epoch 12/15 - Train Loss: 0.0469 - Val Acc: 0.9751\n","Epoch 13/15 - Train Loss: 0.0440 - Val Acc: 0.9760\n","Epoch 14/15 - Train Loss: 0.0368 - Val Acc: 0.9756\n","Epoch 15/15 - Train Loss: 0.0355 - Val Acc: 0.9758\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 0.4490 - Val Acc: 0.9311\n","Epoch 02/15 - Train Loss: 0.2220 - Val Acc: 0.9492\n","Epoch 03/15 - Train Loss: 0.1645 - Val Acc: 0.9565\n","Epoch 04/15 - Train Loss: 0.1323 - Val Acc: 0.9614\n","Epoch 05/15 - Train Loss: 0.1129 - Val Acc: 0.9657\n","Epoch 06/15 - Train Loss: 0.0989 - Val Acc: 0.9692\n","Epoch 07/15 - Train Loss: 0.0852 - Val Acc: 0.9702\n","Epoch 08/15 - Train Loss: 0.0739 - Val Acc: 0.9724\n","Epoch 09/15 - Train Loss: 0.0686 - Val Acc: 0.9722\n","Epoch 10/15 - Train Loss: 0.0617 - Val Acc: 0.9725\n","Epoch 11/15 - Train Loss: 0.0557 - Val Acc: 0.9728\n","Epoch 12/15 - Train Loss: 0.0503 - Val Acc: 0.9733\n","Epoch 13/15 - Train Loss: 0.0462 - Val Acc: 0.9742\n","Epoch 14/15 - Train Loss: 0.0408 - Val Acc: 0.9713\n","Epoch 15/15 - Train Loss: 0.0414 - Val Acc: 0.9748\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 0.4453 - Val Acc: 0.9294\n","Epoch 02/15 - Train Loss: 0.2204 - Val Acc: 0.9502\n","Epoch 03/15 - Train Loss: 0.1644 - Val Acc: 0.9600\n","Epoch 04/15 - Train Loss: 0.1306 - Val Acc: 0.9653\n","Epoch 05/15 - Train Loss: 0.1100 - Val Acc: 0.9671\n","Epoch 06/15 - Train Loss: 0.0934 - Val Acc: 0.9705\n","Epoch 07/15 - Train Loss: 0.0815 - Val Acc: 0.9720\n","Epoch 08/15 - Train Loss: 0.0731 - Val Acc: 0.9729\n","Epoch 09/15 - Train Loss: 0.0653 - Val Acc: 0.9736\n","Epoch 10/15 - Train Loss: 0.0591 - Val Acc: 0.9747\n","Epoch 11/15 - Train Loss: 0.0548 - Val Acc: 0.9731\n","Epoch 12/15 - Train Loss: 0.0477 - Val Acc: 0.9768\n","Epoch 13/15 - Train Loss: 0.0440 - Val Acc: 0.9761\n","Epoch 14/15 - Train Loss: 0.0424 - Val Acc: 0.9758\n","Epoch 15/15 - Train Loss: 0.0377 - Val Acc: 0.9752\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 0.6343 - Val Acc: 0.9173\n","Epoch 02/15 - Train Loss: 0.3221 - Val Acc: 0.9368\n","Epoch 03/15 - Train Loss: 0.2602 - Val Acc: 0.9444\n","Epoch 04/15 - Train Loss: 0.2279 - Val Acc: 0.9522\n","Epoch 05/15 - Train Loss: 0.2023 - Val Acc: 0.9564\n","Epoch 06/15 - Train Loss: 0.1859 - Val Acc: 0.9593\n","Epoch 07/15 - Train Loss: 0.1711 - Val Acc: 0.9607\n","Epoch 08/15 - Train Loss: 0.1599 - Val Acc: 0.9624\n","Epoch 09/15 - Train Loss: 0.1508 - Val Acc: 0.9641\n","Epoch 10/15 - Train Loss: 0.1426 - Val Acc: 0.9659\n","Epoch 11/15 - Train Loss: 0.1377 - Val Acc: 0.9667\n","Epoch 12/15 - Train Loss: 0.1325 - Val Acc: 0.9674\n","Epoch 13/15 - Train Loss: 0.1237 - Val Acc: 0.9684\n","Epoch 14/15 - Train Loss: 0.1194 - Val Acc: 0.9676\n","Epoch 15/15 - Train Loss: 0.1144 - Val Acc: 0.9691\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 0.6439 - Val Acc: 0.9161\n","Epoch 02/15 - Train Loss: 0.3301 - Val Acc: 0.9337\n","Epoch 03/15 - Train Loss: 0.2680 - Val Acc: 0.9446\n","Epoch 04/15 - Train Loss: 0.2316 - Val Acc: 0.9509\n","Epoch 05/15 - Train Loss: 0.2064 - Val Acc: 0.9552\n","Epoch 06/15 - Train Loss: 0.1849 - Val Acc: 0.9577\n","Epoch 07/15 - Train Loss: 0.1750 - Val Acc: 0.9601\n","Epoch 08/15 - Train Loss: 0.1598 - Val Acc: 0.9620\n","Epoch 09/15 - Train Loss: 0.1528 - Val Acc: 0.9641\n","Epoch 10/15 - Train Loss: 0.1445 - Val Acc: 0.9656\n","Epoch 11/15 - Train Loss: 0.1349 - Val Acc: 0.9654\n","Epoch 12/15 - Train Loss: 0.1298 - Val Acc: 0.9677\n","Epoch 13/15 - Train Loss: 0.1254 - Val Acc: 0.9676\n","Epoch 14/15 - Train Loss: 0.1180 - Val Acc: 0.9688\n","Epoch 15/15 - Train Loss: 0.1170 - Val Acc: 0.9688\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 0.6249 - Val Acc: 0.9155\n","Epoch 02/15 - Train Loss: 0.3197 - Val Acc: 0.9361\n","Epoch 03/15 - Train Loss: 0.2616 - Val Acc: 0.9456\n","Epoch 04/15 - Train Loss: 0.2285 - Val Acc: 0.9526\n","Epoch 05/15 - Train Loss: 0.2044 - Val Acc: 0.9560\n","Epoch 06/15 - Train Loss: 0.1882 - Val Acc: 0.9613\n","Epoch 07/15 - Train Loss: 0.1772 - Val Acc: 0.9631\n","Epoch 08/15 - Train Loss: 0.1642 - Val Acc: 0.9645\n","Epoch 09/15 - Train Loss: 0.1482 - Val Acc: 0.9670\n","Epoch 10/15 - Train Loss: 0.1451 - Val Acc: 0.9673\n","Epoch 11/15 - Train Loss: 0.1389 - Val Acc: 0.9687\n","Epoch 12/15 - Train Loss: 0.1321 - Val Acc: 0.9697\n","Epoch 13/15 - Train Loss: 0.1280 - Val Acc: 0.9704\n","Epoch 14/15 - Train Loss: 0.1214 - Val Acc: 0.9711\n","Epoch 15/15 - Train Loss: 0.1142 - Val Acc: 0.9722\n","\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.2628 - Val Acc: 0.6065\n","Epoch 02/15 - Train Loss: 1.5790 - Val Acc: 0.7571\n","Epoch 03/15 - Train Loss: 0.6459 - Val Acc: 0.8530\n","Epoch 04/15 - Train Loss: 0.4631 - Val Acc: 0.8788\n","Epoch 05/15 - Train Loss: 0.3958 - Val Acc: 0.8958\n","Epoch 06/15 - Train Loss: 0.3551 - Val Acc: 0.9039\n","Epoch 07/15 - Train Loss: 0.3255 - Val Acc: 0.9102\n","Epoch 08/15 - Train Loss: 0.3009 - Val Acc: 0.9134\n","Epoch 09/15 - Train Loss: 0.2796 - Val Acc: 0.9213\n","Epoch 10/15 - Train Loss: 0.2591 - Val Acc: 0.9271\n","Epoch 11/15 - Train Loss: 0.2412 - Val Acc: 0.9326\n","Epoch 12/15 - Train Loss: 0.2249 - Val Acc: 0.9348\n","Epoch 13/15 - Train Loss: 0.2096 - Val Acc: 0.9383\n","Epoch 14/15 - Train Loss: 0.1956 - Val Acc: 0.9414\n","Epoch 15/15 - Train Loss: 0.1844 - Val Acc: 0.9435\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 2.2656 - Val Acc: 0.5363\n","Epoch 02/15 - Train Loss: 1.6687 - Val Acc: 0.7671\n","Epoch 03/15 - Train Loss: 0.6948 - Val Acc: 0.8420\n","Epoch 04/15 - Train Loss: 0.4754 - Val Acc: 0.8828\n","Epoch 05/15 - Train Loss: 0.3902 - Val Acc: 0.8966\n","Epoch 06/15 - Train Loss: 0.3476 - Val Acc: 0.9050\n","Epoch 07/15 - Train Loss: 0.3179 - Val Acc: 0.9077\n","Epoch 08/15 - Train Loss: 0.2946 - Val Acc: 0.9152\n","Epoch 09/15 - Train Loss: 0.2735 - Val Acc: 0.9202\n","Epoch 10/15 - Train Loss: 0.2546 - Val Acc: 0.9256\n","Epoch 11/15 - Train Loss: 0.2373 - Val Acc: 0.9284\n","Epoch 12/15 - Train Loss: 0.2210 - Val Acc: 0.9332\n","Epoch 13/15 - Train Loss: 0.2072 - Val Acc: 0.9369\n","Epoch 14/15 - Train Loss: 0.1943 - Val Acc: 0.9395\n","Epoch 15/15 - Train Loss: 0.1815 - Val Acc: 0.9408\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.2705 - Val Acc: 0.5047\n","Epoch 02/15 - Train Loss: 1.7373 - Val Acc: 0.7520\n","Epoch 03/15 - Train Loss: 0.6783 - Val Acc: 0.8468\n","Epoch 04/15 - Train Loss: 0.4618 - Val Acc: 0.8777\n","Epoch 05/15 - Train Loss: 0.3899 - Val Acc: 0.8888\n","Epoch 06/15 - Train Loss: 0.3510 - Val Acc: 0.8997\n","Epoch 07/15 - Train Loss: 0.3224 - Val Acc: 0.9076\n","Epoch 08/15 - Train Loss: 0.2991 - Val Acc: 0.9130\n","Epoch 09/15 - Train Loss: 0.2788 - Val Acc: 0.9180\n","Epoch 10/15 - Train Loss: 0.2589 - Val Acc: 0.9232\n","Epoch 11/15 - Train Loss: 0.2413 - Val Acc: 0.9285\n","Epoch 12/15 - Train Loss: 0.2248 - Val Acc: 0.9335\n","Epoch 13/15 - Train Loss: 0.2095 - Val Acc: 0.9379\n","Epoch 14/15 - Train Loss: 0.1956 - Val Acc: 0.9417\n","Epoch 15/15 - Train Loss: 0.1836 - Val Acc: 0.9446\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 0.3630 - Val Acc: 0.9571\n","Epoch 02/15 - Train Loss: 0.1409 - Val Acc: 0.9645\n","Epoch 03/15 - Train Loss: 0.0993 - Val Acc: 0.9691\n","Epoch 04/15 - Train Loss: 0.0838 - Val Acc: 0.9704\n","Epoch 05/15 - Train Loss: 0.0658 - Val Acc: 0.9745\n","Epoch 06/15 - Train Loss: 0.0596 - Val Acc: 0.9729\n","Epoch 07/15 - Train Loss: 0.0473 - Val Acc: 0.9758\n","Epoch 08/15 - Train Loss: 0.0442 - Val Acc: 0.9741\n","Epoch 09/15 - Train Loss: 0.0419 - Val Acc: 0.9718\n","Epoch 10/15 - Train Loss: 0.0368 - Val Acc: 0.9787\n","Epoch 11/15 - Train Loss: 0.0334 - Val Acc: 0.9784\n","Epoch 12/15 - Train Loss: 0.0322 - Val Acc: 0.9784\n","Epoch 13/15 - Train Loss: 0.0289 - Val Acc: 0.9767\n","Epoch 14/15 - Train Loss: 0.0284 - Val Acc: 0.9762\n","Epoch 15/15 - Train Loss: 0.0246 - Val Acc: 0.9803\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 0.3624 - Val Acc: 0.9509\n","Epoch 02/15 - Train Loss: 0.1440 - Val Acc: 0.9675\n","Epoch 03/15 - Train Loss: 0.1036 - Val Acc: 0.9627\n","Epoch 04/15 - Train Loss: 0.0848 - Val Acc: 0.9729\n","Epoch 05/15 - Train Loss: 0.0665 - Val Acc: 0.9731\n","Epoch 06/15 - Train Loss: 0.0566 - Val Acc: 0.9725\n","Epoch 07/15 - Train Loss: 0.0498 - Val Acc: 0.9752\n","Epoch 08/15 - Train Loss: 0.0449 - Val Acc: 0.9759\n","Epoch 09/15 - Train Loss: 0.0398 - Val Acc: 0.9759\n","Epoch 10/15 - Train Loss: 0.0390 - Val Acc: 0.9769\n","Epoch 11/15 - Train Loss: 0.0322 - Val Acc: 0.9776\n","Epoch 12/15 - Train Loss: 0.0323 - Val Acc: 0.9757\n","Epoch 13/15 - Train Loss: 0.0296 - Val Acc: 0.9795\n","Epoch 14/15 - Train Loss: 0.0257 - Val Acc: 0.9783\n","Epoch 15/15 - Train Loss: 0.0285 - Val Acc: 0.9787\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 0.3646 - Val Acc: 0.9444\n","Epoch 02/15 - Train Loss: 0.1456 - Val Acc: 0.9679\n","Epoch 03/15 - Train Loss: 0.1046 - Val Acc: 0.9718\n","Epoch 04/15 - Train Loss: 0.0832 - Val Acc: 0.9744\n","Epoch 05/15 - Train Loss: 0.0687 - Val Acc: 0.9774\n","Epoch 06/15 - Train Loss: 0.0599 - Val Acc: 0.9729\n","Epoch 07/15 - Train Loss: 0.0520 - Val Acc: 0.9741\n","Epoch 08/15 - Train Loss: 0.0463 - Val Acc: 0.9749\n","Epoch 09/15 - Train Loss: 0.0435 - Val Acc: 0.9783\n","Epoch 10/15 - Train Loss: 0.0387 - Val Acc: 0.9768\n","Epoch 11/15 - Train Loss: 0.0351 - Val Acc: 0.9785\n","Epoch 12/15 - Train Loss: 0.0333 - Val Acc: 0.9749\n","Epoch 13/15 - Train Loss: 0.0320 - Val Acc: 0.9797\n","Epoch 14/15 - Train Loss: 0.0268 - Val Acc: 0.9782\n","Epoch 15/15 - Train Loss: 0.0277 - Val Acc: 0.9785\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 0.6086 - Val Acc: 0.9415\n","Epoch 02/15 - Train Loss: 0.2357 - Val Acc: 0.9583\n","Epoch 03/15 - Train Loss: 0.1857 - Val Acc: 0.9617\n","Epoch 04/15 - Train Loss: 0.1562 - Val Acc: 0.9670\n","Epoch 05/15 - Train Loss: 0.1399 - Val Acc: 0.9698\n","Epoch 06/15 - Train Loss: 0.1194 - Val Acc: 0.9729\n","Epoch 07/15 - Train Loss: 0.1115 - Val Acc: 0.9719\n","Epoch 08/15 - Train Loss: 0.1058 - Val Acc: 0.9727\n","Epoch 09/15 - Train Loss: 0.0968 - Val Acc: 0.9744\n","Epoch 10/15 - Train Loss: 0.0930 - Val Acc: 0.9761\n","Epoch 11/15 - Train Loss: 0.0879 - Val Acc: 0.9782\n","Epoch 12/15 - Train Loss: 0.0809 - Val Acc: 0.9768\n","Epoch 13/15 - Train Loss: 0.0791 - Val Acc: 0.9764\n","Epoch 14/15 - Train Loss: 0.0728 - Val Acc: 0.9777\n","Epoch 15/15 - Train Loss: 0.0717 - Val Acc: 0.9785\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 0.6039 - Val Acc: 0.9399\n","Epoch 02/15 - Train Loss: 0.2361 - Val Acc: 0.9541\n","Epoch 03/15 - Train Loss: 0.1817 - Val Acc: 0.9644\n","Epoch 04/15 - Train Loss: 0.1521 - Val Acc: 0.9698\n","Epoch 05/15 - Train Loss: 0.1330 - Val Acc: 0.9698\n","Epoch 06/15 - Train Loss: 0.1209 - Val Acc: 0.9736\n","Epoch 07/15 - Train Loss: 0.1103 - Val Acc: 0.9730\n","Epoch 08/15 - Train Loss: 0.1016 - Val Acc: 0.9749\n","Epoch 09/15 - Train Loss: 0.0960 - Val Acc: 0.9754\n","Epoch 10/15 - Train Loss: 0.0877 - Val Acc: 0.9748\n","Epoch 11/15 - Train Loss: 0.0852 - Val Acc: 0.9749\n","Epoch 12/15 - Train Loss: 0.0812 - Val Acc: 0.9750\n","Epoch 13/15 - Train Loss: 0.0749 - Val Acc: 0.9764\n","Epoch 14/15 - Train Loss: 0.0720 - Val Acc: 0.9775\n","Epoch 15/15 - Train Loss: 0.0671 - Val Acc: 0.9781\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 0.6018 - Val Acc: 0.9400\n","Epoch 02/15 - Train Loss: 0.2388 - Val Acc: 0.9543\n","Epoch 03/15 - Train Loss: 0.1895 - Val Acc: 0.9633\n","Epoch 04/15 - Train Loss: 0.1534 - Val Acc: 0.9641\n","Epoch 05/15 - Train Loss: 0.1390 - Val Acc: 0.9712\n","Epoch 06/15 - Train Loss: 0.1261 - Val Acc: 0.9706\n","Epoch 07/15 - Train Loss: 0.1135 - Val Acc: 0.9758\n","Epoch 08/15 - Train Loss: 0.1095 - Val Acc: 0.9759\n","Epoch 09/15 - Train Loss: 0.0935 - Val Acc: 0.9764\n","Epoch 10/15 - Train Loss: 0.0892 - Val Acc: 0.9754\n","Epoch 11/15 - Train Loss: 0.0869 - Val Acc: 0.9777\n","Epoch 12/15 - Train Loss: 0.0818 - Val Acc: 0.9787\n","Epoch 13/15 - Train Loss: 0.0768 - Val Acc: 0.9779\n","Epoch 14/15 - Train Loss: 0.0757 - Val Acc: 0.9768\n","Epoch 15/15 - Train Loss: 0.0723 - Val Acc: 0.9799\n","\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.3022 - Val Acc: 0.1142\n","Epoch 02/15 - Train Loss: 2.3013 - Val Acc: 0.1142\n","Epoch 03/15 - Train Loss: 2.3010 - Val Acc: 0.1142\n","Epoch 04/15 - Train Loss: 2.3008 - Val Acc: 0.1142\n","Epoch 05/15 - Train Loss: 2.3005 - Val Acc: 0.1142\n","Epoch 06/15 - Train Loss: 2.3002 - Val Acc: 0.1142\n","Epoch 07/15 - Train Loss: 2.2998 - Val Acc: 0.1142\n","Epoch 08/15 - Train Loss: 2.2991 - Val Acc: 0.1142\n","Epoch 09/15 - Train Loss: 2.2979 - Val Acc: 0.1142\n","Epoch 10/15 - Train Loss: 2.2954 - Val Acc: 0.1142\n","Epoch 11/15 - Train Loss: 2.2880 - Val Acc: 0.1142\n","Epoch 12/15 - Train Loss: 2.2438 - Val Acc: 0.2310\n","Epoch 13/15 - Train Loss: 1.8847 - Val Acc: 0.3090\n","Epoch 14/15 - Train Loss: 1.5239 - Val Acc: 0.4924\n","Epoch 15/15 - Train Loss: 1.1480 - Val Acc: 0.6916\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 2.3037 - Val Acc: 0.0994\n","Epoch 02/15 - Train Loss: 2.3012 - Val Acc: 0.1112\n","Epoch 03/15 - Train Loss: 2.3006 - Val Acc: 0.1112\n","Epoch 04/15 - Train Loss: 2.3002 - Val Acc: 0.1112\n","Epoch 05/15 - Train Loss: 2.2999 - Val Acc: 0.1112\n","Epoch 06/15 - Train Loss: 2.2994 - Val Acc: 0.1112\n","Epoch 07/15 - Train Loss: 2.2988 - Val Acc: 0.1112\n","Epoch 08/15 - Train Loss: 2.2978 - Val Acc: 0.1112\n","Epoch 09/15 - Train Loss: 2.2959 - Val Acc: 0.1112\n","Epoch 10/15 - Train Loss: 2.2913 - Val Acc: 0.1112\n","Epoch 11/15 - Train Loss: 2.2727 - Val Acc: 0.2667\n","Epoch 12/15 - Train Loss: 2.0795 - Val Acc: 0.2840\n","Epoch 13/15 - Train Loss: 1.6108 - Val Acc: 0.4576\n","Epoch 14/15 - Train Loss: 1.2580 - Val Acc: 0.6956\n","Epoch 15/15 - Train Loss: 0.7645 - Val Acc: 0.8112\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.3033 - Val Acc: 0.0955\n","Epoch 02/15 - Train Loss: 2.3016 - Val Acc: 0.1117\n","Epoch 03/15 - Train Loss: 2.3011 - Val Acc: 0.1117\n","Epoch 04/15 - Train Loss: 2.3009 - Val Acc: 0.1117\n","Epoch 05/15 - Train Loss: 2.3008 - Val Acc: 0.1117\n","Epoch 06/15 - Train Loss: 2.3006 - Val Acc: 0.1117\n","Epoch 07/15 - Train Loss: 2.3005 - Val Acc: 0.1117\n","Epoch 08/15 - Train Loss: 2.3002 - Val Acc: 0.1117\n","Epoch 09/15 - Train Loss: 2.2999 - Val Acc: 0.1117\n","Epoch 10/15 - Train Loss: 2.2995 - Val Acc: 0.1117\n","Epoch 11/15 - Train Loss: 2.2988 - Val Acc: 0.1117\n","Epoch 12/15 - Train Loss: 2.2978 - Val Acc: 0.1117\n","Epoch 13/15 - Train Loss: 2.2959 - Val Acc: 0.1117\n","Epoch 14/15 - Train Loss: 2.2914 - Val Acc: 0.1462\n","Epoch 15/15 - Train Loss: 2.2758 - Val Acc: 0.2057\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 0.5914 - Val Acc: 0.9464\n","Epoch 02/15 - Train Loss: 0.2122 - Val Acc: 0.9599\n","Epoch 03/15 - Train Loss: 0.1590 - Val Acc: 0.9656\n","Epoch 04/15 - Train Loss: 0.1360 - Val Acc: 0.9661\n","Epoch 05/15 - Train Loss: 0.1215 - Val Acc: 0.9699\n","Epoch 06/15 - Train Loss: 0.1043 - Val Acc: 0.9741\n","Epoch 07/15 - Train Loss: 0.0910 - Val Acc: 0.9720\n","Epoch 08/15 - Train Loss: 0.0853 - Val Acc: 0.9734\n","Epoch 09/15 - Train Loss: 0.0823 - Val Acc: 0.9747\n","Epoch 10/15 - Train Loss: 0.0773 - Val Acc: 0.9742\n","Epoch 11/15 - Train Loss: 0.0653 - Val Acc: 0.9747\n","Epoch 12/15 - Train Loss: 0.0582 - Val Acc: 0.9694\n","Epoch 13/15 - Train Loss: 0.0689 - Val Acc: 0.9741\n","Epoch 14/15 - Train Loss: 0.0578 - Val Acc: 0.9777\n","Epoch 15/15 - Train Loss: 0.0607 - Val Acc: 0.9754\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 0.5381 - Val Acc: 0.9458\n","Epoch 02/15 - Train Loss: 0.2135 - Val Acc: 0.9608\n","Epoch 03/15 - Train Loss: 0.1571 - Val Acc: 0.9596\n","Epoch 04/15 - Train Loss: 0.1342 - Val Acc: 0.9637\n","Epoch 05/15 - Train Loss: 0.1244 - Val Acc: 0.9706\n","Epoch 06/15 - Train Loss: 0.0993 - Val Acc: 0.9594\n","Epoch 07/15 - Train Loss: 0.0958 - Val Acc: 0.9755\n","Epoch 08/15 - Train Loss: 0.0778 - Val Acc: 0.9526\n","Epoch 09/15 - Train Loss: 0.0795 - Val Acc: 0.9758\n","Epoch 10/15 - Train Loss: 0.0765 - Val Acc: 0.9733\n","Epoch 11/15 - Train Loss: 0.0660 - Val Acc: 0.9783\n","Epoch 12/15 - Train Loss: 0.0639 - Val Acc: 0.9725\n","Epoch 13/15 - Train Loss: 0.0605 - Val Acc: 0.9745\n","Epoch 14/15 - Train Loss: 0.0625 - Val Acc: 0.9766\n","Epoch 15/15 - Train Loss: 0.0606 - Val Acc: 0.9762\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 0.6360 - Val Acc: 0.9374\n","Epoch 02/15 - Train Loss: 0.2226 - Val Acc: 0.9585\n","Epoch 03/15 - Train Loss: 0.1652 - Val Acc: 0.9578\n","Epoch 04/15 - Train Loss: 0.1457 - Val Acc: 0.9649\n","Epoch 05/15 - Train Loss: 0.1156 - Val Acc: 0.9746\n","Epoch 06/15 - Train Loss: 0.1042 - Val Acc: 0.9719\n","Epoch 07/15 - Train Loss: 0.0910 - Val Acc: 0.9705\n","Epoch 08/15 - Train Loss: 0.0856 - Val Acc: 0.9725\n","Epoch 09/15 - Train Loss: 0.0845 - Val Acc: 0.9739\n","Epoch 10/15 - Train Loss: 0.0782 - Val Acc: 0.9742\n","Epoch 11/15 - Train Loss: 0.0802 - Val Acc: 0.9778\n","Epoch 12/15 - Train Loss: 0.0532 - Val Acc: 0.9789\n","Epoch 13/15 - Train Loss: 0.0585 - Val Acc: 0.9775\n","Epoch 14/15 - Train Loss: 0.0629 - Val Acc: 0.9795\n","Epoch 15/15 - Train Loss: 0.0491 - Val Acc: 0.9712\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.3516 - Val Acc: 0.7812\n","Epoch 02/15 - Train Loss: 0.4767 - Val Acc: 0.9444\n","Epoch 03/15 - Train Loss: 0.3081 - Val Acc: 0.9536\n","Epoch 04/15 - Train Loss: 0.2581 - Val Acc: 0.9591\n","Epoch 05/15 - Train Loss: 0.2258 - Val Acc: 0.9673\n","Epoch 06/15 - Train Loss: 0.2097 - Val Acc: 0.9661\n","Epoch 07/15 - Train Loss: 0.1929 - Val Acc: 0.9673\n","Epoch 08/15 - Train Loss: 0.1750 - Val Acc: 0.9701\n","Epoch 09/15 - Train Loss: 0.1742 - Val Acc: 0.9684\n","Epoch 10/15 - Train Loss: 0.1489 - Val Acc: 0.9730\n","Epoch 11/15 - Train Loss: 0.1502 - Val Acc: 0.9698\n","Epoch 12/15 - Train Loss: 0.1387 - Val Acc: 0.9744\n","Epoch 13/15 - Train Loss: 0.1242 - Val Acc: 0.9749\n","Epoch 14/15 - Train Loss: 0.1366 - Val Acc: 0.9726\n","Epoch 15/15 - Train Loss: 0.1273 - Val Acc: 0.9771\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.5104 - Val Acc: 0.6878\n","Epoch 02/15 - Train Loss: 0.7090 - Val Acc: 0.8505\n","Epoch 03/15 - Train Loss: 0.4926 - Val Acc: 0.8669\n","Epoch 04/15 - Train Loss: 0.3513 - Val Acc: 0.9539\n","Epoch 05/15 - Train Loss: 0.2818 - Val Acc: 0.9605\n","Epoch 06/15 - Train Loss: 0.2382 - Val Acc: 0.9647\n","Epoch 07/15 - Train Loss: 0.2218 - Val Acc: 0.9671\n","Epoch 08/15 - Train Loss: 0.1947 - Val Acc: 0.9671\n","Epoch 09/15 - Train Loss: 0.1812 - Val Acc: 0.9683\n","Epoch 10/15 - Train Loss: 0.1696 - Val Acc: 0.9708\n","Epoch 11/15 - Train Loss: 0.1508 - Val Acc: 0.9706\n","Epoch 12/15 - Train Loss: 0.1592 - Val Acc: 0.9732\n","Epoch 13/15 - Train Loss: 0.1455 - Val Acc: 0.9742\n","Epoch 14/15 - Train Loss: 0.1403 - Val Acc: 0.9733\n","Epoch 15/15 - Train Loss: 0.1267 - Val Acc: 0.9736\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.4357 - Val Acc: 0.6882\n","Epoch 02/15 - Train Loss: 0.5994 - Val Acc: 0.8905\n","Epoch 03/15 - Train Loss: 0.3794 - Val Acc: 0.9559\n","Epoch 04/15 - Train Loss: 0.2872 - Val Acc: 0.9597\n","Epoch 05/15 - Train Loss: 0.2536 - Val Acc: 0.9614\n","Epoch 06/15 - Train Loss: 0.2157 - Val Acc: 0.9619\n","Epoch 07/15 - Train Loss: 0.1951 - Val Acc: 0.9665\n","Epoch 08/15 - Train Loss: 0.1865 - Val Acc: 0.9685\n","Epoch 09/15 - Train Loss: 0.1716 - Val Acc: 0.9704\n","Epoch 10/15 - Train Loss: 0.1559 - Val Acc: 0.9720\n","Epoch 11/15 - Train Loss: 0.1481 - Val Acc: 0.9726\n","Epoch 12/15 - Train Loss: 0.1419 - Val Acc: 0.9725\n","Epoch 13/15 - Train Loss: 0.1351 - Val Acc: 0.9748\n","Epoch 14/15 - Train Loss: 0.1339 - Val Acc: 0.9726\n","Epoch 15/15 - Train Loss: 0.1287 - Val Acc: 0.9746\n","\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.9602 - Val Acc: 0.3730\n","Epoch 02/15 - Train Loss: 1.7403 - Val Acc: 0.4100\n","Epoch 03/15 - Train Loss: 1.6535 - Val Acc: 0.4311\n","Epoch 04/15 - Train Loss: 1.5981 - Val Acc: 0.4465\n","Epoch 05/15 - Train Loss: 1.5559 - Val Acc: 0.4573\n","Epoch 06/15 - Train Loss: 1.5215 - Val Acc: 0.4634\n","Epoch 07/15 - Train Loss: 1.4919 - Val Acc: 0.4749\n","Epoch 08/15 - Train Loss: 1.4640 - Val Acc: 0.4787\n","Epoch 09/15 - Train Loss: 1.4386 - Val Acc: 0.4842\n","Epoch 10/15 - Train Loss: 1.4143 - Val Acc: 0.4868\n","Epoch 11/15 - Train Loss: 1.3916 - Val Acc: 0.4919\n","Epoch 12/15 - Train Loss: 1.3688 - Val Acc: 0.4933\n","Epoch 13/15 - Train Loss: 1.3473 - Val Acc: 0.4965\n","Epoch 14/15 - Train Loss: 1.3282 - Val Acc: 0.5010\n","Epoch 15/15 - Train Loss: 1.3085 - Val Acc: 0.5010\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.9383 - Val Acc: 0.3769\n","Epoch 02/15 - Train Loss: 1.7257 - Val Acc: 0.4078\n","Epoch 03/15 - Train Loss: 1.6405 - Val Acc: 0.4324\n","Epoch 04/15 - Train Loss: 1.5843 - Val Acc: 0.4398\n","Epoch 05/15 - Train Loss: 1.5422 - Val Acc: 0.4479\n","Epoch 06/15 - Train Loss: 1.5071 - Val Acc: 0.4544\n","Epoch 07/15 - Train Loss: 1.4765 - Val Acc: 0.4609\n","Epoch 08/15 - Train Loss: 1.4489 - Val Acc: 0.4660\n","Epoch 09/15 - Train Loss: 1.4234 - Val Acc: 0.4707\n","Epoch 10/15 - Train Loss: 1.3993 - Val Acc: 0.4787\n","Epoch 11/15 - Train Loss: 1.3757 - Val Acc: 0.4850\n","Epoch 12/15 - Train Loss: 1.3540 - Val Acc: 0.4862\n","Epoch 13/15 - Train Loss: 1.3329 - Val Acc: 0.4874\n","Epoch 14/15 - Train Loss: 1.3125 - Val Acc: 0.4892\n","Epoch 15/15 - Train Loss: 1.2940 - Val Acc: 0.4965\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.9437 - Val Acc: 0.3702\n","Epoch 02/15 - Train Loss: 1.7308 - Val Acc: 0.4035\n","Epoch 03/15 - Train Loss: 1.6448 - Val Acc: 0.4257\n","Epoch 04/15 - Train Loss: 1.5893 - Val Acc: 0.4387\n","Epoch 05/15 - Train Loss: 1.5467 - Val Acc: 0.4476\n","Epoch 06/15 - Train Loss: 1.5118 - Val Acc: 0.4566\n","Epoch 07/15 - Train Loss: 1.4818 - Val Acc: 0.4655\n","Epoch 08/15 - Train Loss: 1.4540 - Val Acc: 0.4696\n","Epoch 09/15 - Train Loss: 1.4285 - Val Acc: 0.4725\n","Epoch 10/15 - Train Loss: 1.4036 - Val Acc: 0.4805\n","Epoch 11/15 - Train Loss: 1.3807 - Val Acc: 0.4819\n","Epoch 12/15 - Train Loss: 1.3589 - Val Acc: 0.4870\n","Epoch 13/15 - Train Loss: 1.3382 - Val Acc: 0.4897\n","Epoch 14/15 - Train Loss: 1.3184 - Val Acc: 0.4925\n","Epoch 15/15 - Train Loss: 1.2991 - Val Acc: 0.4964\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.7559 - Val Acc: 0.4372\n","Epoch 02/15 - Train Loss: 1.5840 - Val Acc: 0.4750\n","Epoch 03/15 - Train Loss: 1.5130 - Val Acc: 0.4840\n","Epoch 04/15 - Train Loss: 1.4701 - Val Acc: 0.4852\n","Epoch 05/15 - Train Loss: 1.4317 - Val Acc: 0.4903\n","Epoch 06/15 - Train Loss: 1.4030 - Val Acc: 0.4938\n","Epoch 07/15 - Train Loss: 1.3729 - Val Acc: 0.5067\n","Epoch 08/15 - Train Loss: 1.3373 - Val Acc: 0.4980\n","Epoch 09/15 - Train Loss: 1.3189 - Val Acc: 0.5086\n","Epoch 10/15 - Train Loss: 1.2986 - Val Acc: 0.5123\n","Epoch 11/15 - Train Loss: 1.2683 - Val Acc: 0.5103\n","Epoch 12/15 - Train Loss: 1.2552 - Val Acc: 0.5097\n","Epoch 13/15 - Train Loss: 1.2209 - Val Acc: 0.5162\n","Epoch 14/15 - Train Loss: 1.2156 - Val Acc: 0.5171\n","Epoch 15/15 - Train Loss: 1.1943 - Val Acc: 0.5156\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.7431 - Val Acc: 0.4318\n","Epoch 02/15 - Train Loss: 1.5715 - Val Acc: 0.4643\n","Epoch 03/15 - Train Loss: 1.5065 - Val Acc: 0.4582\n","Epoch 04/15 - Train Loss: 1.4577 - Val Acc: 0.4771\n","Epoch 05/15 - Train Loss: 1.4188 - Val Acc: 0.4819\n","Epoch 06/15 - Train Loss: 1.3860 - Val Acc: 0.4865\n","Epoch 07/15 - Train Loss: 1.3535 - Val Acc: 0.4852\n","Epoch 08/15 - Train Loss: 1.3263 - Val Acc: 0.4945\n","Epoch 09/15 - Train Loss: 1.2987 - Val Acc: 0.5052\n","Epoch 10/15 - Train Loss: 1.2767 - Val Acc: 0.5008\n","Epoch 11/15 - Train Loss: 1.2656 - Val Acc: 0.5047\n","Epoch 12/15 - Train Loss: 1.2352 - Val Acc: 0.4963\n","Epoch 13/15 - Train Loss: 1.2091 - Val Acc: 0.5051\n","Epoch 14/15 - Train Loss: 1.1945 - Val Acc: 0.5021\n","Epoch 15/15 - Train Loss: 1.1883 - Val Acc: 0.5063\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.7444 - Val Acc: 0.4258\n","Epoch 02/15 - Train Loss: 1.5809 - Val Acc: 0.4680\n","Epoch 03/15 - Train Loss: 1.5118 - Val Acc: 0.4767\n","Epoch 04/15 - Train Loss: 1.4647 - Val Acc: 0.4804\n","Epoch 05/15 - Train Loss: 1.4240 - Val Acc: 0.4828\n","Epoch 06/15 - Train Loss: 1.3891 - Val Acc: 0.4762\n","Epoch 07/15 - Train Loss: 1.3595 - Val Acc: 0.4992\n","Epoch 08/15 - Train Loss: 1.3385 - Val Acc: 0.5010\n","Epoch 09/15 - Train Loss: 1.3076 - Val Acc: 0.4942\n","Epoch 10/15 - Train Loss: 1.2906 - Val Acc: 0.5017\n","Epoch 11/15 - Train Loss: 1.2623 - Val Acc: 0.4971\n","Epoch 12/15 - Train Loss: 1.2470 - Val Acc: 0.5064\n","Epoch 13/15 - Train Loss: 1.2201 - Val Acc: 0.5041\n","Epoch 14/15 - Train Loss: 1.1956 - Val Acc: 0.5020\n","Epoch 15/15 - Train Loss: 1.1822 - Val Acc: 0.5077\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.8646 - Val Acc: 0.4267\n","Epoch 02/15 - Train Loss: 1.7072 - Val Acc: 0.4491\n","Epoch 03/15 - Train Loss: 1.6627 - Val Acc: 0.4694\n","Epoch 04/15 - Train Loss: 1.6217 - Val Acc: 0.4722\n","Epoch 05/15 - Train Loss: 1.6036 - Val Acc: 0.4780\n","Epoch 06/15 - Train Loss: 1.5810 - Val Acc: 0.4799\n","Epoch 07/15 - Train Loss: 1.5620 - Val Acc: 0.4864\n","Epoch 08/15 - Train Loss: 1.5474 - Val Acc: 0.4916\n","Epoch 09/15 - Train Loss: 1.5315 - Val Acc: 0.4955\n","Epoch 10/15 - Train Loss: 1.5166 - Val Acc: 0.4888\n","Epoch 11/15 - Train Loss: 1.5069 - Val Acc: 0.4913\n","Epoch 12/15 - Train Loss: 1.4931 - Val Acc: 0.5004\n","Epoch 13/15 - Train Loss: 1.4825 - Val Acc: 0.4963\n","Epoch 14/15 - Train Loss: 1.4726 - Val Acc: 0.5037\n","Epoch 15/15 - Train Loss: 1.4695 - Val Acc: 0.5015\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.8501 - Val Acc: 0.4166\n","Epoch 02/15 - Train Loss: 1.7073 - Val Acc: 0.4471\n","Epoch 03/15 - Train Loss: 1.6546 - Val Acc: 0.4564\n","Epoch 04/15 - Train Loss: 1.6180 - Val Acc: 0.4612\n","Epoch 05/15 - Train Loss: 1.5955 - Val Acc: 0.4647\n","Epoch 06/15 - Train Loss: 1.5770 - Val Acc: 0.4795\n","Epoch 07/15 - Train Loss: 1.5514 - Val Acc: 0.4730\n","Epoch 08/15 - Train Loss: 1.5453 - Val Acc: 0.4817\n","Epoch 09/15 - Train Loss: 1.5151 - Val Acc: 0.4825\n","Epoch 10/15 - Train Loss: 1.5087 - Val Acc: 0.4792\n","Epoch 11/15 - Train Loss: 1.5002 - Val Acc: 0.4789\n","Epoch 12/15 - Train Loss: 1.4811 - Val Acc: 0.4865\n","Epoch 13/15 - Train Loss: 1.4753 - Val Acc: 0.4876\n","Epoch 14/15 - Train Loss: 1.4639 - Val Acc: 0.4943\n","Epoch 15/15 - Train Loss: 1.4579 - Val Acc: 0.4918\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.8563 - Val Acc: 0.4218\n","Epoch 02/15 - Train Loss: 1.6994 - Val Acc: 0.4420\n","Epoch 03/15 - Train Loss: 1.6517 - Val Acc: 0.4605\n","Epoch 04/15 - Train Loss: 1.6222 - Val Acc: 0.4618\n","Epoch 05/15 - Train Loss: 1.5894 - Val Acc: 0.4681\n","Epoch 06/15 - Train Loss: 1.5800 - Val Acc: 0.4711\n","Epoch 07/15 - Train Loss: 1.5597 - Val Acc: 0.4791\n","Epoch 08/15 - Train Loss: 1.5360 - Val Acc: 0.4812\n","Epoch 09/15 - Train Loss: 1.5228 - Val Acc: 0.4858\n","Epoch 10/15 - Train Loss: 1.5135 - Val Acc: 0.4903\n","Epoch 11/15 - Train Loss: 1.4965 - Val Acc: 0.4878\n","Epoch 12/15 - Train Loss: 1.4885 - Val Acc: 0.4933\n","Epoch 13/15 - Train Loss: 1.4800 - Val Acc: 0.4947\n","Epoch 14/15 - Train Loss: 1.4707 - Val Acc: 0.4893\n","Epoch 15/15 - Train Loss: 1.4555 - Val Acc: 0.4975\n","\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.2508 - Val Acc: 0.2462\n","Epoch 02/15 - Train Loss: 2.0308 - Val Acc: 0.3103\n","Epoch 03/15 - Train Loss: 1.8626 - Val Acc: 0.3596\n","Epoch 04/15 - Train Loss: 1.7548 - Val Acc: 0.3854\n","Epoch 05/15 - Train Loss: 1.6761 - Val Acc: 0.4104\n","Epoch 06/15 - Train Loss: 1.6148 - Val Acc: 0.4241\n","Epoch 07/15 - Train Loss: 1.5613 - Val Acc: 0.4433\n","Epoch 08/15 - Train Loss: 1.5103 - Val Acc: 0.4554\n","Epoch 09/15 - Train Loss: 1.4627 - Val Acc: 0.4693\n","Epoch 10/15 - Train Loss: 1.4181 - Val Acc: 0.4821\n","Epoch 11/15 - Train Loss: 1.3775 - Val Acc: 0.4845\n","Epoch 12/15 - Train Loss: 1.3363 - Val Acc: 0.4898\n","Epoch 13/15 - Train Loss: 1.2981 - Val Acc: 0.5033\n","Epoch 14/15 - Train Loss: 1.2584 - Val Acc: 0.5063\n","Epoch 15/15 - Train Loss: 1.2222 - Val Acc: 0.5022\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 2.2625 - Val Acc: 0.2291\n","Epoch 02/15 - Train Loss: 2.0661 - Val Acc: 0.2770\n","Epoch 03/15 - Train Loss: 1.8991 - Val Acc: 0.3493\n","Epoch 04/15 - Train Loss: 1.7820 - Val Acc: 0.3800\n","Epoch 05/15 - Train Loss: 1.6864 - Val Acc: 0.4011\n","Epoch 06/15 - Train Loss: 1.6157 - Val Acc: 0.4183\n","Epoch 07/15 - Train Loss: 1.5575 - Val Acc: 0.4348\n","Epoch 08/15 - Train Loss: 1.5043 - Val Acc: 0.4499\n","Epoch 09/15 - Train Loss: 1.4558 - Val Acc: 0.4630\n","Epoch 10/15 - Train Loss: 1.4093 - Val Acc: 0.4716\n","Epoch 11/15 - Train Loss: 1.3673 - Val Acc: 0.4760\n","Epoch 12/15 - Train Loss: 1.3276 - Val Acc: 0.4874\n","Epoch 13/15 - Train Loss: 1.2863 - Val Acc: 0.4891\n","Epoch 14/15 - Train Loss: 1.2517 - Val Acc: 0.5023\n","Epoch 15/15 - Train Loss: 1.2148 - Val Acc: 0.5046\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.2550 - Val Acc: 0.2330\n","Epoch 02/15 - Train Loss: 2.0427 - Val Acc: 0.2995\n","Epoch 03/15 - Train Loss: 1.8711 - Val Acc: 0.3486\n","Epoch 04/15 - Train Loss: 1.7513 - Val Acc: 0.3883\n","Epoch 05/15 - Train Loss: 1.6648 - Val Acc: 0.4145\n","Epoch 06/15 - Train Loss: 1.5998 - Val Acc: 0.4336\n","Epoch 07/15 - Train Loss: 1.5473 - Val Acc: 0.4447\n","Epoch 08/15 - Train Loss: 1.4973 - Val Acc: 0.4605\n","Epoch 09/15 - Train Loss: 1.4502 - Val Acc: 0.4725\n","Epoch 10/15 - Train Loss: 1.4044 - Val Acc: 0.4794\n","Epoch 11/15 - Train Loss: 1.3612 - Val Acc: 0.4885\n","Epoch 12/15 - Train Loss: 1.3195 - Val Acc: 0.4900\n","Epoch 13/15 - Train Loss: 1.2792 - Val Acc: 0.4946\n","Epoch 14/15 - Train Loss: 1.2429 - Val Acc: 0.5041\n","Epoch 15/15 - Train Loss: 1.2026 - Val Acc: 0.5113\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.7877 - Val Acc: 0.4162\n","Epoch 02/15 - Train Loss: 1.6088 - Val Acc: 0.4624\n","Epoch 03/15 - Train Loss: 1.5321 - Val Acc: 0.4640\n","Epoch 04/15 - Train Loss: 1.4678 - Val Acc: 0.4841\n","Epoch 05/15 - Train Loss: 1.4201 - Val Acc: 0.4934\n","Epoch 06/15 - Train Loss: 1.3713 - Val Acc: 0.4987\n","Epoch 07/15 - Train Loss: 1.3270 - Val Acc: 0.5090\n","Epoch 08/15 - Train Loss: 1.2846 - Val Acc: 0.5049\n","Epoch 09/15 - Train Loss: 1.2554 - Val Acc: 0.5148\n","Epoch 10/15 - Train Loss: 1.2156 - Val Acc: 0.5193\n","Epoch 11/15 - Train Loss: 1.1832 - Val Acc: 0.5225\n","Epoch 12/15 - Train Loss: 1.1451 - Val Acc: 0.5139\n","Epoch 13/15 - Train Loss: 1.1176 - Val Acc: 0.5196\n","Epoch 14/15 - Train Loss: 1.0946 - Val Acc: 0.5253\n","Epoch 15/15 - Train Loss: 1.0681 - Val Acc: 0.5209\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.7831 - Val Acc: 0.4223\n","Epoch 02/15 - Train Loss: 1.6026 - Val Acc: 0.4586\n","Epoch 03/15 - Train Loss: 1.5185 - Val Acc: 0.4706\n","Epoch 04/15 - Train Loss: 1.4548 - Val Acc: 0.4805\n","Epoch 05/15 - Train Loss: 1.3946 - Val Acc: 0.4912\n","Epoch 06/15 - Train Loss: 1.3478 - Val Acc: 0.4912\n","Epoch 07/15 - Train Loss: 1.3062 - Val Acc: 0.4988\n","Epoch 08/15 - Train Loss: 1.2722 - Val Acc: 0.5058\n","Epoch 09/15 - Train Loss: 1.2291 - Val Acc: 0.5044\n","Epoch 10/15 - Train Loss: 1.2017 - Val Acc: 0.5163\n","Epoch 11/15 - Train Loss: 1.1664 - Val Acc: 0.5156\n","Epoch 12/15 - Train Loss: 1.1302 - Val Acc: 0.5145\n","Epoch 13/15 - Train Loss: 1.1007 - Val Acc: 0.5157\n","Epoch 14/15 - Train Loss: 1.0733 - Val Acc: 0.5166\n","Epoch 15/15 - Train Loss: 1.0535 - Val Acc: 0.5172\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.7828 - Val Acc: 0.4332\n","Epoch 02/15 - Train Loss: 1.6087 - Val Acc: 0.4438\n","Epoch 03/15 - Train Loss: 1.5177 - Val Acc: 0.4782\n","Epoch 04/15 - Train Loss: 1.4554 - Val Acc: 0.4875\n","Epoch 05/15 - Train Loss: 1.4042 - Val Acc: 0.4882\n","Epoch 06/15 - Train Loss: 1.3576 - Val Acc: 0.4934\n","Epoch 07/15 - Train Loss: 1.3131 - Val Acc: 0.5052\n","Epoch 08/15 - Train Loss: 1.2772 - Val Acc: 0.5095\n","Epoch 09/15 - Train Loss: 1.2396 - Val Acc: 0.5123\n","Epoch 10/15 - Train Loss: 1.2033 - Val Acc: 0.5115\n","Epoch 11/15 - Train Loss: 1.1699 - Val Acc: 0.5186\n","Epoch 12/15 - Train Loss: 1.1419 - Val Acc: 0.5159\n","Epoch 13/15 - Train Loss: 1.1096 - Val Acc: 0.5181\n","Epoch 14/15 - Train Loss: 1.0850 - Val Acc: 0.5180\n","Epoch 15/15 - Train Loss: 1.0504 - Val Acc: 0.5248\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [512, 256, 128] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 1.9533 - Val Acc: 0.3845\n","Epoch 02/15 - Train Loss: 1.8065 - Val Acc: 0.4151\n","Epoch 03/15 - Train Loss: 1.7420 - Val Acc: 0.4289\n","Epoch 04/15 - Train Loss: 1.7054 - Val Acc: 0.4474\n","Epoch 05/15 - Train Loss: 1.6698 - Val Acc: 0.4508\n","Epoch 06/15 - Train Loss: 1.6481 - Val Acc: 0.4442\n","Epoch 07/15 - Train Loss: 1.6240 - Val Acc: 0.4656\n","Epoch 08/15 - Train Loss: 1.6088 - Val Acc: 0.4645\n","Epoch 09/15 - Train Loss: 1.5854 - Val Acc: 0.4721\n","Epoch 10/15 - Train Loss: 1.5692 - Val Acc: 0.4804\n","Epoch 11/15 - Train Loss: 1.5553 - Val Acc: 0.4815\n","Epoch 12/15 - Train Loss: 1.5384 - Val Acc: 0.4843\n","Epoch 13/15 - Train Loss: 1.5198 - Val Acc: 0.4856\n","Epoch 14/15 - Train Loss: 1.5048 - Val Acc: 0.4852\n","Epoch 15/15 - Train Loss: 1.4953 - Val Acc: 0.4834\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.9527 - Val Acc: 0.3892\n","Epoch 02/15 - Train Loss: 1.7893 - Val Acc: 0.4187\n","Epoch 03/15 - Train Loss: 1.7316 - Val Acc: 0.4257\n","Epoch 04/15 - Train Loss: 1.6886 - Val Acc: 0.4519\n","Epoch 05/15 - Train Loss: 1.6597 - Val Acc: 0.4526\n","Epoch 06/15 - Train Loss: 1.6319 - Val Acc: 0.4576\n","Epoch 07/15 - Train Loss: 1.6097 - Val Acc: 0.4606\n","Epoch 08/15 - Train Loss: 1.5893 - Val Acc: 0.4690\n","Epoch 09/15 - Train Loss: 1.5720 - Val Acc: 0.4686\n","Epoch 10/15 - Train Loss: 1.5560 - Val Acc: 0.4709\n","Epoch 11/15 - Train Loss: 1.5414 - Val Acc: 0.4776\n","Epoch 12/15 - Train Loss: 1.5205 - Val Acc: 0.4771\n","Epoch 13/15 - Train Loss: 1.5028 - Val Acc: 0.4847\n","Epoch 14/15 - Train Loss: 1.4981 - Val Acc: 0.4831\n","Epoch 15/15 - Train Loss: 1.4729 - Val Acc: 0.4946\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 1.9626 - Val Acc: 0.3744\n","Epoch 02/15 - Train Loss: 1.8049 - Val Acc: 0.4130\n","Epoch 03/15 - Train Loss: 1.7369 - Val Acc: 0.4199\n","Epoch 04/15 - Train Loss: 1.7002 - Val Acc: 0.4352\n","Epoch 05/15 - Train Loss: 1.6704 - Val Acc: 0.4512\n","Epoch 06/15 - Train Loss: 1.6435 - Val Acc: 0.4599\n","Epoch 07/15 - Train Loss: 1.6175 - Val Acc: 0.4713\n","Epoch 08/15 - Train Loss: 1.5968 - Val Acc: 0.4698\n","Epoch 09/15 - Train Loss: 1.5843 - Val Acc: 0.4813\n","Epoch 10/15 - Train Loss: 1.5534 - Val Acc: 0.4768\n","Epoch 11/15 - Train Loss: 1.5413 - Val Acc: 0.4749\n","Epoch 12/15 - Train Loss: 1.5252 - Val Acc: 0.4836\n","Epoch 13/15 - Train Loss: 1.5234 - Val Acc: 0.4830\n","Epoch 14/15 - Train Loss: 1.5049 - Val Acc: 0.4864\n","Epoch 15/15 - Train Loss: 1.4845 - Val Acc: 0.4902\n","\n","=== Config: {'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.3046 - Val Acc: 0.0998\n","Epoch 02/15 - Train Loss: 2.3031 - Val Acc: 0.1004\n","Epoch 03/15 - Train Loss: 2.3027 - Val Acc: 0.1211\n","Epoch 04/15 - Train Loss: 2.3025 - Val Acc: 0.0993\n","Epoch 05/15 - Train Loss: 2.3024 - Val Acc: 0.1625\n","Epoch 06/15 - Train Loss: 2.3023 - Val Acc: 0.1051\n","Epoch 07/15 - Train Loss: 2.3022 - Val Acc: 0.1202\n","Epoch 08/15 - Train Loss: 2.3021 - Val Acc: 0.0982\n","Epoch 09/15 - Train Loss: 2.3020 - Val Acc: 0.0982\n","Epoch 10/15 - Train Loss: 2.3019 - Val Acc: 0.0982\n","Epoch 11/15 - Train Loss: 2.3017 - Val Acc: 0.0983\n","Epoch 12/15 - Train Loss: 2.3014 - Val Acc: 0.1026\n","Epoch 13/15 - Train Loss: 2.3010 - Val Acc: 0.1352\n","Epoch 14/15 - Train Loss: 2.3004 - Val Acc: 0.1443\n","Epoch 15/15 - Train Loss: 2.2993 - Val Acc: 0.1637\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 2.3040 - Val Acc: 0.0975\n","Epoch 02/15 - Train Loss: 2.3028 - Val Acc: 0.0975\n","Epoch 03/15 - Train Loss: 2.3024 - Val Acc: 0.1243\n","Epoch 04/15 - Train Loss: 2.3022 - Val Acc: 0.1144\n","Epoch 05/15 - Train Loss: 2.3020 - Val Acc: 0.1333\n","Epoch 06/15 - Train Loss: 2.3018 - Val Acc: 0.0976\n","Epoch 07/15 - Train Loss: 2.3015 - Val Acc: 0.1228\n","Epoch 08/15 - Train Loss: 2.3011 - Val Acc: 0.1167\n","Epoch 09/15 - Train Loss: 2.3004 - Val Acc: 0.1708\n","Epoch 10/15 - Train Loss: 2.2991 - Val Acc: 0.1720\n","Epoch 11/15 - Train Loss: 2.2967 - Val Acc: 0.1829\n","Epoch 12/15 - Train Loss: 2.2909 - Val Acc: 0.1810\n","Epoch 13/15 - Train Loss: 2.2743 - Val Acc: 0.1763\n","Epoch 14/15 - Train Loss: 2.2121 - Val Acc: 0.1786\n","Epoch 15/15 - Train Loss: 2.0774 - Val Acc: 0.1965\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.3045 - Val Acc: 0.1012\n","Epoch 02/15 - Train Loss: 2.3030 - Val Acc: 0.1012\n","Epoch 03/15 - Train Loss: 2.3025 - Val Acc: 0.0959\n","Epoch 04/15 - Train Loss: 2.3023 - Val Acc: 0.0959\n","Epoch 05/15 - Train Loss: 2.3021 - Val Acc: 0.0959\n","Epoch 06/15 - Train Loss: 2.3019 - Val Acc: 0.0959\n","Epoch 07/15 - Train Loss: 2.3017 - Val Acc: 0.0959\n","Epoch 08/15 - Train Loss: 2.3014 - Val Acc: 0.0959\n","Epoch 09/15 - Train Loss: 2.3008 - Val Acc: 0.0959\n","Epoch 10/15 - Train Loss: 2.2999 - Val Acc: 0.1033\n","Epoch 11/15 - Train Loss: 2.2981 - Val Acc: 0.1610\n","Epoch 12/15 - Train Loss: 2.2939 - Val Acc: 0.1646\n","Epoch 13/15 - Train Loss: 2.2794 - Val Acc: 0.1661\n","Epoch 14/15 - Train Loss: 2.1994 - Val Acc: 0.1796\n","Epoch 15/15 - Train Loss: 2.0740 - Val Acc: 0.2023\n","\n","=== Config: {'lr': 0.001, 'batch_size': 64, 'optimizer': 'Adam', 'dropout': 0.2}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.0024 - Val Acc: 0.2905\n","Epoch 02/15 - Train Loss: 1.8599 - Val Acc: 0.3377\n","Epoch 03/15 - Train Loss: 1.7667 - Val Acc: 0.3780\n","Epoch 04/15 - Train Loss: 1.6951 - Val Acc: 0.3836\n","Epoch 05/15 - Train Loss: 1.6398 - Val Acc: 0.4106\n","Epoch 06/15 - Train Loss: 1.6027 - Val Acc: 0.4283\n","Epoch 07/15 - Train Loss: 1.5507 - Val Acc: 0.4484\n","Epoch 08/15 - Train Loss: 1.5046 - Val Acc: 0.4595\n","Epoch 09/15 - Train Loss: 1.4772 - Val Acc: 0.4690\n","Epoch 10/15 - Train Loss: 1.4489 - Val Acc: 0.4684\n","Epoch 11/15 - Train Loss: 1.4143 - Val Acc: 0.4776\n","Epoch 12/15 - Train Loss: 1.3834 - Val Acc: 0.4773\n","Epoch 13/15 - Train Loss: 1.3624 - Val Acc: 0.4850\n","Epoch 14/15 - Train Loss: 1.3458 - Val Acc: 0.4907\n","Epoch 15/15 - Train Loss: 1.3109 - Val Acc: 0.4846\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 1.9609 - Val Acc: 0.3315\n","Epoch 02/15 - Train Loss: 1.7795 - Val Acc: 0.3781\n","Epoch 03/15 - Train Loss: 1.6876 - Val Acc: 0.3853\n","Epoch 04/15 - Train Loss: 1.6222 - Val Acc: 0.4139\n","Epoch 05/15 - Train Loss: 1.5755 - Val Acc: 0.4347\n","Epoch 06/15 - Train Loss: 1.5280 - Val Acc: 0.4321\n","Epoch 07/15 - Train Loss: 1.4807 - Val Acc: 0.4560\n","Epoch 08/15 - Train Loss: 1.4502 - Val Acc: 0.4553\n","Epoch 09/15 - Train Loss: 1.4168 - Val Acc: 0.4640\n","Epoch 10/15 - Train Loss: 1.3775 - Val Acc: 0.4686\n","Epoch 11/15 - Train Loss: 1.3499 - Val Acc: 0.4772\n","Epoch 12/15 - Train Loss: 1.3292 - Val Acc: 0.4751\n","Epoch 13/15 - Train Loss: 1.2983 - Val Acc: 0.4734\n","Epoch 14/15 - Train Loss: 1.2740 - Val Acc: 0.4743\n","Epoch 15/15 - Train Loss: 1.2676 - Val Acc: 0.4804\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.0186 - Val Acc: 0.2943\n","Epoch 02/15 - Train Loss: 1.8326 - Val Acc: 0.3541\n","Epoch 03/15 - Train Loss: 1.7340 - Val Acc: 0.3868\n","Epoch 04/15 - Train Loss: 1.6785 - Val Acc: 0.4069\n","Epoch 05/15 - Train Loss: 1.6215 - Val Acc: 0.4180\n","Epoch 06/15 - Train Loss: 1.5762 - Val Acc: 0.4431\n","Epoch 07/15 - Train Loss: 1.5316 - Val Acc: 0.4501\n","Epoch 08/15 - Train Loss: 1.4943 - Val Acc: 0.4578\n","Epoch 09/15 - Train Loss: 1.4526 - Val Acc: 0.4606\n","Epoch 10/15 - Train Loss: 1.4175 - Val Acc: 0.4602\n","Epoch 11/15 - Train Loss: 1.4005 - Val Acc: 0.4693\n","Epoch 12/15 - Train Loss: 1.3631 - Val Acc: 0.4615\n","Epoch 13/15 - Train Loss: 1.3436 - Val Acc: 0.4904\n","Epoch 14/15 - Train Loss: 1.3393 - Val Acc: 0.4818\n","Epoch 15/15 - Train Loss: 1.3073 - Val Acc: 0.4849\n","\n","=== Config: {'lr': 0.001, 'batch_size': 128, 'optimizer': 'Adam', 'dropout': 0.5}, Architecture: [4096, 2048, 1024, 512, 256, 128, 64] ===\n","\n","--- Fold 1/3 ---\n","Epoch 01/15 - Train Loss: 2.1702 - Val Acc: 0.2339\n","Epoch 02/15 - Train Loss: 2.0043 - Val Acc: 0.2548\n","Epoch 03/15 - Train Loss: 1.9618 - Val Acc: 0.2677\n","Epoch 04/15 - Train Loss: 1.9351 - Val Acc: 0.2455\n","Epoch 05/15 - Train Loss: 1.9228 - Val Acc: 0.2647\n","Epoch 06/15 - Train Loss: 1.9082 - Val Acc: 0.2934\n","Epoch 07/15 - Train Loss: 1.8995 - Val Acc: 0.2936\n","Epoch 08/15 - Train Loss: 1.8872 - Val Acc: 0.2959\n","Epoch 09/15 - Train Loss: 1.8860 - Val Acc: 0.2807\n","Epoch 10/15 - Train Loss: 1.8802 - Val Acc: 0.2905\n","Epoch 11/15 - Train Loss: 1.8743 - Val Acc: 0.2867\n","Epoch 12/15 - Train Loss: 1.8742 - Val Acc: 0.2952\n","Epoch 13/15 - Train Loss: 1.8565 - Val Acc: 0.2900\n","Epoch 14/15 - Train Loss: 1.8611 - Val Acc: 0.2868\n","Epoch 15/15 - Train Loss: 1.8551 - Val Acc: 0.3010\n","\n","--- Fold 2/3 ---\n","Epoch 01/15 - Train Loss: 2.1454 - Val Acc: 0.2462\n","Epoch 02/15 - Train Loss: 2.0015 - Val Acc: 0.2563\n","Epoch 03/15 - Train Loss: 1.9639 - Val Acc: 0.2693\n","Epoch 04/15 - Train Loss: 1.9344 - Val Acc: 0.2647\n","Epoch 05/15 - Train Loss: 1.9179 - Val Acc: 0.2696\n","Epoch 06/15 - Train Loss: 1.9010 - Val Acc: 0.2892\n","Epoch 07/15 - Train Loss: 1.8885 - Val Acc: 0.2934\n","Epoch 08/15 - Train Loss: 1.8805 - Val Acc: 0.2867\n","Epoch 09/15 - Train Loss: 1.8679 - Val Acc: 0.2920\n","Epoch 10/15 - Train Loss: 1.8679 - Val Acc: 0.3025\n","Epoch 11/15 - Train Loss: 1.8558 - Val Acc: 0.2875\n","Epoch 12/15 - Train Loss: 1.8672 - Val Acc: 0.3034\n","Epoch 13/15 - Train Loss: 1.8514 - Val Acc: 0.3131\n","Epoch 14/15 - Train Loss: 1.8505 - Val Acc: 0.3055\n","Epoch 15/15 - Train Loss: 1.8494 - Val Acc: 0.2982\n","\n","--- Fold 3/3 ---\n","Epoch 01/15 - Train Loss: 2.1480 - Val Acc: 0.2504\n","Epoch 02/15 - Train Loss: 2.0054 - Val Acc: 0.2571\n","Epoch 03/15 - Train Loss: 1.9591 - Val Acc: 0.2827\n","Epoch 04/15 - Train Loss: 1.9314 - Val Acc: 0.2983\n","Epoch 05/15 - Train Loss: 1.9074 - Val Acc: 0.3055\n","Epoch 06/15 - Train Loss: 1.8848 - Val Acc: 0.3133\n","Epoch 07/15 - Train Loss: 1.8731 - Val Acc: 0.3055\n","Epoch 08/15 - Train Loss: 1.8609 - Val Acc: 0.3106\n","Epoch 09/15 - Train Loss: 1.8509 - Val Acc: 0.3098\n","Epoch 10/15 - Train Loss: 1.8546 - Val Acc: 0.3202\n","Epoch 11/15 - Train Loss: 1.8363 - Val Acc: 0.3111\n","Epoch 12/15 - Train Loss: 1.8289 - Val Acc: 0.3273\n","Epoch 13/15 - Train Loss: 1.8268 - Val Acc: 0.3096\n","Epoch 14/15 - Train Loss: 1.8248 - Val Acc: 0.3198\n","Epoch 15/15 - Train Loss: 1.8134 - Val Acc: 0.3137\n","\n","MNIST Results:\n","                            architecture  \\\n","0                                  [128]   \n","1                                  [128]   \n","2                                  [128]   \n","3                        [512, 256, 128]   \n","4                        [512, 256, 128]   \n","5                        [512, 256, 128]   \n","6  [4096, 2048, 1024, 512, 256, 128, 64]   \n","7  [4096, 2048, 1024, 512, 256, 128, 64]   \n","8  [4096, 2048, 1024, 512, 256, 128, 64]   \n","\n","                                              config  mean_acc   std_acc  \\\n","0  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...  0.924800  0.001476   \n","1  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...  0.975267  0.000452   \n","2  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...  0.970033  0.001574   \n","3  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...  0.942983  0.001614   \n","4  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...  0.979150  0.000815   \n","5  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...  0.978817  0.000779   \n","6  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...  0.569517  0.261807   \n","7  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...  0.974267  0.002193   \n","8  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...  0.975100  0.001501   \n","\n","   runtime_sec  \n","0   287.267176  \n","1   301.361373  \n","2   272.205589  \n","3   300.147645  \n","4   318.651584  \n","5   281.078878  \n","6   383.270886  \n","7   468.275732  \n","8   374.636354  \n","\n","CIFAR-10 Results:\n","                            architecture  \\\n","0                                  [128]   \n","1                                  [128]   \n","2                                  [128]   \n","3                        [512, 256, 128]   \n","4                        [512, 256, 128]   \n","5                        [512, 256, 128]   \n","6  [4096, 2048, 1024, 512, 256, 128, 64]   \n","7  [4096, 2048, 1024, 512, 256, 128, 64]   \n","8  [4096, 2048, 1024, 512, 256, 128, 64]   \n","\n","                                              config  mean_acc   std_acc  \\\n","0  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...   0.49796  0.002143   \n","1  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...   0.50988  0.004103   \n","2  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...   0.49694  0.004015   \n","3  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...   0.50604  0.003839   \n","4  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...   0.52096  0.003099   \n","5  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...   0.48940  0.004593   \n","6  {'lr': 0.01, 'batch_size': 64, 'optimizer': 'S...   0.18750  0.017013   \n","7  {'lr': 0.001, 'batch_size': 64, 'optimizer': '...   0.48326  0.002062   \n","8  {'lr': 0.001, 'batch_size': 128, 'optimizer': ...   0.30428  0.006750   \n","\n","   runtime_sec  \n","0   517.036499  \n","1   522.361284  \n","2   498.982522  \n","3   531.261492  \n","4   550.165645  \n","5   513.392825  \n","6   641.815964  \n","7   756.058280  \n","8   665.356551  \n"]}]},{"cell_type":"code","source":["print(mnist_df.loc[0, 'config'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGIBA6dTFdAH","executionInfo":{"status":"ok","timestamp":1745100373302,"user_tz":300,"elapsed":12,"user":{"displayName":"jerin","userId":"05872963528617007031"}},"outputId":"00ca7c25-7293-4616-e6ee-8e9f6ac1a250"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'lr': 0.01, 'batch_size': 64, 'optimizer': 'SGD', 'dropout': 0.0}\n"]}]}]}